{
  "analytic": "{\"langSmith\":{\"credentialId\":\"9fb56a6f-2c15-486c-9125-6fd27091d002\",\"projectName\":\"INTERSEAS - Lista Negativa NCM SC\",\"status\":true}}",
  "apiConfig": null,
  "apikeyid": null,
  "category": "Prod",
  "chatbotConfig": null,
  "createdDate": "2024-12-16T14:28:59.274Z",
  "deployed": false,
  "flowData": "{\"nodes\":[{\"id\":\"seqStart_0\",\"position\":{\"x\":21.45392570982736,\"y\":340.13358779696284},\"type\":\"customNode\",\"data\":{\"id\":\"seqStart_0\",\"label\":\"Start\",\"version\":2,\"name\":\"seqStart\",\"type\":\"Start\",\"baseClasses\":[\"Start\"],\"category\":\"Sequential Agents\",\"description\":\"Starting point of the conversation\",\"inputParams\":[],\"inputAnchors\":[{\"label\":\"Chat Model\",\"name\":\"model\",\"type\":\"BaseChatModel\",\"description\":\"Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat\",\"id\":\"seqStart_0-input-model-BaseChatModel\"},{\"label\":\"Agent Memory\",\"name\":\"agentMemory\",\"type\":\"BaseCheckpointSaver\",\"description\":\"Save the state of the agent\",\"optional\":true,\"id\":\"seqStart_0-input-agentMemory-BaseCheckpointSaver\"},{\"label\":\"State\",\"name\":\"state\",\"type\":\"State\",\"description\":\"State is an object that is updated by nodes in the graph, passing from one node to another. By default, state contains \\\"messages\\\" that got updated with each message sent and received.\",\"optional\":true,\"id\":\"seqStart_0-input-state-State\"},{\"label\":\"Input Moderation\",\"description\":\"Detect text that could generate harmful output and prevent it from being sent to the language model\",\"name\":\"inputModeration\",\"type\":\"Moderation\",\"optional\":true,\"list\":true,\"id\":\"seqStart_0-input-inputModeration-Moderation\"}],\"inputs\":{\"model\":\"{{chatOpenAI_0.data.instance}}\",\"agentMemory\":\"\",\"state\":\"\",\"inputModeration\":\"\"},\"outputAnchors\":[{\"id\":\"seqStart_0-output-seqStart-Start\",\"name\":\"seqStart\",\"label\":\"Start\",\"description\":\"Starting point of the conversation\",\"type\":\"Start\"}],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":383,\"positionAbsolute\":{\"x\":21.45392570982736,\"y\":340.13358779696284},\"selected\":false,\"dragging\":false},{\"id\":\"seqEnd_0\",\"position\":{\"x\":1144.8448148439054,\"y\":573.7304138781866},\"type\":\"customNode\",\"data\":{\"id\":\"seqEnd_0\",\"label\":\"End\",\"version\":2,\"name\":\"seqEnd\",\"type\":\"End\",\"baseClasses\":[\"End\"],\"category\":\"Sequential Agents\",\"description\":\"End conversation\",\"inputParams\":[],\"inputAnchors\":[{\"label\":\"Agent | Condition | LLM | Tool Node\",\"name\":\"sequentialNode\",\"type\":\"Agent | Condition | LLMNode | ToolNode\",\"id\":\"seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode\"}],\"inputs\":{\"sequentialNode\":\"{{seqLLMNode_1.data.instance}}\"},\"outputAnchors\":[],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":143,\"selected\":false,\"positionAbsolute\":{\"x\":1144.8448148439054,\"y\":573.7304138781866},\"dragging\":false},{\"id\":\"seqLLMNode_0\",\"position\":{\"x\":395.3651580311179,\"y\":267.5533381510083},\"type\":\"customNode\",\"data\":{\"id\":\"seqLLMNode_0\",\"label\":\"LLM Node\",\"version\":3,\"name\":\"seqLLMNode\",\"type\":\"LLMNode\",\"baseClasses\":[\"LLMNode\"],\"category\":\"Sequential Agents\",\"description\":\"Run Chat Model and return the output\",\"inputParams\":[{\"label\":\"Name\",\"name\":\"llmNodeName\",\"type\":\"string\",\"placeholder\":\"LLM\",\"id\":\"seqLLMNode_0-input-llmNodeName-string\"},{\"label\":\"System Prompt\",\"name\":\"systemMessagePrompt\",\"type\":\"string\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-systemMessagePrompt-string\"},{\"label\":\"Human Prompt\",\"name\":\"humanMessagePrompt\",\"type\":\"string\",\"description\":\"This prompt will be added at the end of the messages as human message\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-humanMessagePrompt-string\"},{\"label\":\"Messages History\",\"name\":\"messageHistory\",\"description\":\"Return a list of messages between System Prompt and Human Prompt. This is useful when you want to provide few shot examples\",\"type\":\"code\",\"hideCodeExecute\":true,\"codeExample\":\"const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\\n\\nreturn [\\n    new HumanMessage(\\\"What is 333382 ü¶ú 1932?\\\"),\\n    new AIMessage({\\n        content: \\\"\\\",\\n        tool_calls: [\\n        {\\n            id: \\\"12345\\\",\\n            name: \\\"calulator\\\",\\n            args: {\\n                number1: 333382,\\n                number2: 1932,\\n                operation: \\\"divide\\\",\\n            },\\n        },\\n        ],\\n    }),\\n    new ToolMessage({\\n        tool_call_id: \\\"12345\\\",\\n        content: \\\"The answer is 172.558.\\\",\\n    }),\\n    new AIMessage(\\\"The answer is 172.558.\\\"),\\n]\",\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-messageHistory-code\"},{\"label\":\"Format Prompt Values\",\"name\":\"promptValues\",\"description\":\"Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value\",\"type\":\"json\",\"optional\":true,\"acceptVariable\":true,\"list\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-promptValues-json\"},{\"label\":\"JSON Structured Output\",\"name\":\"llmStructuredOutput\",\"type\":\"datagrid\",\"description\":\"Instruct the LLM to give output in a JSON structured schema\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"editable\":true},{\"field\":\"type\",\"headerName\":\"Type\",\"type\":\"singleSelect\",\"valueOptions\":[\"String\",\"String Array\",\"Number\",\"Boolean\",\"Enum\"],\"editable\":true},{\"field\":\"enumValues\",\"headerName\":\"Enum Values\",\"editable\":true},{\"field\":\"description\",\"headerName\":\"Description\",\"flex\":1,\"editable\":true}],\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-llmStructuredOutput-datagrid\"},{\"label\":\"Update State\",\"name\":\"updateStateMemory\",\"type\":\"tabs\",\"tabIdentifier\":\"selectedUpdateStateMemoryTab\",\"default\":\"updateStateMemoryUI\",\"additionalParams\":true,\"tabs\":[{\"label\":\"Update State (Table)\",\"name\":\"updateStateMemoryUI\",\"type\":\"datagrid\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Key and value pair to be updated. For example: if you have the following State:\\n    | Key       | Operation     | Default Value     |\\n    |-----------|---------------|-------------------|\\n    | user      | Replace       |                   |\\n\\n    You can update the \\\"user\\\" value with the following:\\n    | Key       | Value     |\\n    |-----------|-----------|\\n    | user      | john doe  |\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can do the following:\\n    | Key       | Value                     |\\n    |-----------|---------------------------|\\n    | user      | `$flow.output.content`  |\\n\\n3. You can get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"type\":\"asyncSingleSelect\",\"loadMethod\":\"loadStateKeys\",\"flex\":0.5,\"editable\":true},{\"field\":\"value\",\"headerName\":\"Value\",\"type\":\"freeSolo\",\"valueOptions\":[{\"label\":\"LLM Node Output (string)\",\"value\":\"$flow.output.content\"},{\"label\":\"LLM JSON Output Key (string)\",\"value\":\"$flow.output.<replace-with-key>\"},{\"label\":\"Global variable (string)\",\"value\":\"$vars.<variable-name>\"},{\"label\":\"Input Question (string)\",\"value\":\"$flow.input\"},{\"label\":\"Session Id (string)\",\"value\":\"$flow.sessionId\"},{\"label\":\"Chat Id (string)\",\"value\":\"$flow.chatId\"},{\"label\":\"Chatflow Id (string)\",\"value\":\"$flow.chatflowId\"}],\"editable\":true,\"flex\":1}],\"optional\":true,\"additionalParams\":true},{\"label\":\"Update State (Code)\",\"name\":\"updateStateMemoryCode\",\"type\":\"code\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Return the key value JSON object. For example: if you have the following State:\\n    ```json\\n    {\\n        \\\"user\\\": null\\n    }\\n    ```\\n\\n    You can update the \\\"user\\\" value by returning the following:\\n    ```js\\n    return {\\n        \\\"user\\\": \\\"john doe\\\"\\n    }\\n    ```\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can return the following:\\n    ```js\\n    return {\\n        \\\"user\\\": $flow.output.content\\n    }\\n    ```\\n\\n3. You can also get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values. Must return an object representing the state\",\"hideCodeExecute\":true,\"codeExample\":\"const result = $flow.output;\\n\\n/* Suppose we have a custom State schema like this:\\n* {\\n    aggregate: {\\n        value: (x, y) => x.concat(y),\\n        default: () => []\\n    }\\n  }\\n*/\\n\\nreturn {\\n  aggregate: [result.content]\\n};\",\"optional\":true,\"additionalParams\":true}],\"id\":\"seqLLMNode_0-input-updateStateMemory-tabs\"}],\"inputAnchors\":[{\"label\":\"Start | Agent | Condition | LLM | Tool Node\",\"name\":\"sequentialNode\",\"type\":\"Start | Agent | Condition | LLMNode | ToolNode\",\"list\":true,\"id\":\"seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"label\":\"Chat Model\",\"name\":\"model\",\"type\":\"BaseChatModel\",\"optional\":true,\"description\":\"Overwrite model to be used for this node\",\"id\":\"seqLLMNode_0-input-model-BaseChatModel\"}],\"inputs\":{\"llmNodeName\":\"agent_pesquisador\",\"systemMessagePrompt\":\"**Fun√ß√£o do Agente:**  \\nSua tarefa √© analisar o texto recebido na entrada `{question}`, extra√≠do de um decreto, e identificar **todos os NCMs mencionados** na **Lista de Mercadorias Importadas N√£o Alcan√ßadas por Benef√≠cios Fiscais**, separando-os em **valores √∫nicos** e **intervalos**, conforme necess√°rio.  \\n\\n1. **Identifica√ß√£o de NCMs:**  \\n   - Considere como NCM apenas sequ√™ncias que atendam aos seguintes crit√©rios:  \\n     - **Possuem 4 ou mais d√≠gitos consecutivos**, com ou sem pontos como separadores.  \\n     - **Devem ser tratados como um √∫nico NCM**, mesmo que contenham pontos.  \\n     - Exemplos v√°lidos:  \\n       - `\\\"1001\\\"`, `\\\"10.25\\\"`, `\\\"1001.25\\\"`, `\\\"70.13\\\"`, `\\\"2710.9\\\"`, `\\\"3826.00.00\\\"`.  \\n     - Exemplos inv√°lidos (**NUNCA devem ser considerados ou retornados**):  \\n       - `\\\"1\\\"`, `\\\"12\\\"`, `\\\"123\\\"`, `\\\"12.3\\\"`, `\\\"1.23\\\"` (menos de 4 d√≠gitos consecutivos).  \\n   - Identifique **intervalos de NCMs** quando dois valores v√°lidos forem conectados por palavras como `\\\"a\\\"`, `\\\"at√©\\\"`, ou similares.  \\n     - Para considerar um intervalo v√°lido:  \\n       - Ambos os limites do intervalo devem ser sequ√™ncias v√°lidas de NCMs (com 4 ou mais d√≠gitos).  \\n       - O texto deve conter termos como `\\\"NCM\\\"`, `\\\"NCMs\\\"` ou express√µes similares, antes ou depois do intervalo.  \\n     - Exemplos v√°lidos de intervalos:  \\n       - `\\\"1001 a 1006\\\"`, `\\\"10.25 a 10.50\\\"`, `\\\"70.13 a 70.20\\\"`, `\\\"2710.9 a 2711.5\\\"`, `\\\"3826.00.00 a 3826.00.05\\\"`.  \\n     - Exemplos inv√°lidos de intervalos (n√£o devem ser considerados):  \\n       - `\\\"1 a 3\\\"`, `\\\"12 a 15\\\"`, `\\\"17 a 19\\\"` (valores com menos de 4 d√≠gitos consecutivos).  \\n\\n2. **Classifica√ß√£o:**  \\n   - Separe os NCMs encontrados em dois grupos:  \\n     - **UNICO**: Apenas NCMs √∫nicos (n√£o presentes dentro de intervalos).  \\n     - **INTERVALO**: Apenas intervalos de NCMs.  \\n   - **N√£o duplique informa√ß√µes**:  \\n     - Se um NCM √∫nico estiver contido em um intervalo, liste apenas o intervalo em \\\"INTERVALO\\\".  \\n\\n3. **Regras de Formata√ß√£o do Retorno:**  \\n   - O retorno deve ser no formato JSON, seguindo a estrutura:  \\n     ```json\\n     (\\n       \\\"UNICO\\\": [\\\"1001\\\", \\\"10.25\\\"],\\n       \\\"INTERVALO\\\": [\\\"1003 a 1006\\\"]\\n     )\\n     ```  \\n   - Todos os valores devem ser strings, utilizando aspas duplas (`\\\"`).  \\n   - O retorno deve ser v√°lido em JSON e **n√£o incluir quebras de linha** ou textos adicionais.  \\n\\n4. **Regras Adicionais:**  \\n   - **NUNCA** divida ou modifique intervalos encontrados; liste-os exatamente como aparecem.  \\n   - **NUNCA** retorne valores de NCMs repetidos\\n   - Caso nenhum NCM seja encontrado, retorne:  \\n     ```json\\n     (\\n       \\\"UNICO\\\": [],\\n       \\\"INTERVALO\\\": []\\n     )\\n     ```  \\n   - N√£o inclua textos explicativos, coment√°rios ou informa√ß√µes adicionais no retorno.  \\n\\n5. **Objetivo Final:**  \\n   - Extraia todos os NCMs mencionados na lista do decreto.  \\n   - Garanta que o retorno contenha exclusivamente NCMs classificados entre **valores √∫nicos** e **intervalos**, respeitando as regras de formata√ß√£o e valida√ß√£o.\",\"humanMessagePrompt\":\"Retorne a lista de NCMs encontrados\",\"messageHistory\":\"\",\"sequentialNode\":[\"{{seqStart_0.data.instance}}\"],\"model\":\"\",\"promptValues\":\"{\\\"question\\\":\\\"{{question}}\\\"}\",\"llmStructuredOutput\":\"\",\"updateStateMemory\":\"updateStateMemoryUI\"},\"outputAnchors\":[{\"id\":\"seqLLMNode_0-output-seqLLMNode-LLMNode\",\"name\":\"seqLLMNode\",\"label\":\"LLMNode\",\"description\":\"Run Chat Model and return the output\",\"type\":\"LLMNode\"}],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":451,\"selected\":false,\"positionAbsolute\":{\"x\":395.3651580311179,\"y\":267.5533381510083},\"dragging\":false},{\"id\":\"seqLLMNode_1\",\"position\":{\"x\":770.2872467476096,\"y\":265.7318010896166},\"type\":\"customNode\",\"data\":{\"id\":\"seqLLMNode_1\",\"label\":\"LLM Node\",\"version\":3,\"name\":\"seqLLMNode\",\"type\":\"LLMNode\",\"baseClasses\":[\"LLMNode\"],\"category\":\"Sequential Agents\",\"description\":\"Run Chat Model and return the output\",\"inputParams\":[{\"label\":\"Name\",\"name\":\"llmNodeName\",\"type\":\"string\",\"placeholder\":\"LLM\",\"id\":\"seqLLMNode_1-input-llmNodeName-string\"},{\"label\":\"System Prompt\",\"name\":\"systemMessagePrompt\",\"type\":\"string\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-systemMessagePrompt-string\"},{\"label\":\"Human Prompt\",\"name\":\"humanMessagePrompt\",\"type\":\"string\",\"description\":\"This prompt will be added at the end of the messages as human message\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-humanMessagePrompt-string\"},{\"label\":\"Messages History\",\"name\":\"messageHistory\",\"description\":\"Return a list of messages between System Prompt and Human Prompt. This is useful when you want to provide few shot examples\",\"type\":\"code\",\"hideCodeExecute\":true,\"codeExample\":\"const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\\n\\nreturn [\\n    new HumanMessage(\\\"What is 333382 ü¶ú 1932?\\\"),\\n    new AIMessage({\\n        content: \\\"\\\",\\n        tool_calls: [\\n        {\\n            id: \\\"12345\\\",\\n            name: \\\"calulator\\\",\\n            args: {\\n                number1: 333382,\\n                number2: 1932,\\n                operation: \\\"divide\\\",\\n            },\\n        },\\n        ],\\n    }),\\n    new ToolMessage({\\n        tool_call_id: \\\"12345\\\",\\n        content: \\\"The answer is 172.558.\\\",\\n    }),\\n    new AIMessage(\\\"The answer is 172.558.\\\"),\\n]\",\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-messageHistory-code\"},{\"label\":\"Format Prompt Values\",\"name\":\"promptValues\",\"description\":\"Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value\",\"type\":\"json\",\"optional\":true,\"acceptVariable\":true,\"list\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-promptValues-json\"},{\"label\":\"JSON Structured Output\",\"name\":\"llmStructuredOutput\",\"type\":\"datagrid\",\"description\":\"Instruct the LLM to give output in a JSON structured schema\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"editable\":true},{\"field\":\"type\",\"headerName\":\"Type\",\"type\":\"singleSelect\",\"valueOptions\":[\"String\",\"String Array\",\"Number\",\"Boolean\",\"Enum\"],\"editable\":true},{\"field\":\"enumValues\",\"headerName\":\"Enum Values\",\"editable\":true},{\"field\":\"description\",\"headerName\":\"Description\",\"flex\":1,\"editable\":true}],\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-llmStructuredOutput-datagrid\"},{\"label\":\"Update State\",\"name\":\"updateStateMemory\",\"type\":\"tabs\",\"tabIdentifier\":\"selectedUpdateStateMemoryTab\",\"default\":\"updateStateMemoryUI\",\"additionalParams\":true,\"tabs\":[{\"label\":\"Update State (Table)\",\"name\":\"updateStateMemoryUI\",\"type\":\"datagrid\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Key and value pair to be updated. For example: if you have the following State:\\n    | Key       | Operation     | Default Value     |\\n    |-----------|---------------|-------------------|\\n    | user      | Replace       |                   |\\n\\n    You can update the \\\"user\\\" value with the following:\\n    | Key       | Value     |\\n    |-----------|-----------|\\n    | user      | john doe  |\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can do the following:\\n    | Key       | Value                     |\\n    |-----------|---------------------------|\\n    | user      | `$flow.output.content`  |\\n\\n3. You can get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"type\":\"asyncSingleSelect\",\"loadMethod\":\"loadStateKeys\",\"flex\":0.5,\"editable\":true},{\"field\":\"value\",\"headerName\":\"Value\",\"type\":\"freeSolo\",\"valueOptions\":[{\"label\":\"LLM Node Output (string)\",\"value\":\"$flow.output.content\"},{\"label\":\"LLM JSON Output Key (string)\",\"value\":\"$flow.output.<replace-with-key>\"},{\"label\":\"Global variable (string)\",\"value\":\"$vars.<variable-name>\"},{\"label\":\"Input Question (string)\",\"value\":\"$flow.input\"},{\"label\":\"Session Id (string)\",\"value\":\"$flow.sessionId\"},{\"label\":\"Chat Id (string)\",\"value\":\"$flow.chatId\"},{\"label\":\"Chatflow Id (string)\",\"value\":\"$flow.chatflowId\"}],\"editable\":true,\"flex\":1}],\"optional\":true,\"additionalParams\":true},{\"label\":\"Update State (Code)\",\"name\":\"updateStateMemoryCode\",\"type\":\"code\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Return the key value JSON object. For example: if you have the following State:\\n    ```json\\n    {\\n        \\\"user\\\": null\\n    }\\n    ```\\n\\n    You can update the \\\"user\\\" value by returning the following:\\n    ```js\\n    return {\\n        \\\"user\\\": \\\"john doe\\\"\\n    }\\n    ```\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can return the following:\\n    ```js\\n    return {\\n        \\\"user\\\": $flow.output.content\\n    }\\n    ```\\n\\n3. You can also get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values. Must return an object representing the state\",\"hideCodeExecute\":true,\"codeExample\":\"const result = $flow.output;\\n\\n/* Suppose we have a custom State schema like this:\\n* {\\n    aggregate: {\\n        value: (x, y) => x.concat(y),\\n        default: () => []\\n    }\\n  }\\n*/\\n\\nreturn {\\n  aggregate: [result.content]\\n};\",\"optional\":true,\"additionalParams\":true}],\"id\":\"seqLLMNode_1-input-updateStateMemory-tabs\"}],\"inputAnchors\":[{\"label\":\"Start | Agent | Condition | LLM | Tool Node\",\"name\":\"sequentialNode\",\"type\":\"Start | Agent | Condition | LLMNode | ToolNode\",\"list\":true,\"id\":\"seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"label\":\"Chat Model\",\"name\":\"model\",\"type\":\"BaseChatModel\",\"optional\":true,\"description\":\"Overwrite model to be used for this node\",\"id\":\"seqLLMNode_1-input-model-BaseChatModel\"}],\"inputs\":{\"llmNodeName\":\"agent\",\"systemMessagePrompt\":\"**Fun√ß√£o do Agente:**  \\nSua tarefa √© analisar a entrada: {response} e processar **todos os NCMs** mencionados, transformando intervalos em listas de valores individuais e gerando um retorno consolidado.  \\n\\n1. **Identifica√ß√£o de NCMs:**  \\n   - Os NCMs podem ser:  \\n     - **Valores √∫nicos**: como `\\\"1001\\\"`.  \\n     - **Intervalos**: como `\\\"5622 a 5626\\\"`.  \\n   - Identifique intervalos que contenham dois n√∫meros separados pela palavra `\\\"a\\\"`.  \\n\\n2. **Transforma√ß√£o de Intervalos:**  \\n   - Para cada intervalo identificado (exemplo: `\\\"5622 a 5626\\\"`):  \\n     - Extraia os dois n√∫meros (in√≠cio e fim).  \\n     - Gere uma sequ√™ncia de todos os n√∫meros do intervalo, incluindo o in√≠cio e o fim (exemplo: `\\\"5622 a 5626\\\"` ‚Üí `[\\\"5622\\\", \\\"5623\\\", \\\"5624\\\", \\\"5625\\\", \\\"5626\\\"]`).  \\n     - Substitua o intervalo original pela sequ√™ncia gerada.  \\n   - **Nunca mantenha intervalos no formato original no retorno.**  \\n\\n3. **Consolida√ß√£o dos Resultados:**  \\n   - Combine os valores √∫nicos com as listas expandidas dos intervalos em uma √∫nica lista consolidada.  \\n   - Garanta que a lista resultante esteja em ordem crescente (ordenada numericamente).  \\n\\n4. **Regras de Retorno:**  \\n   - O retorno deve ser no formato JSON, com a seguinte estrutura:  \\n     ```json\\n     (\\\"NCM\\\": [\\\"1001\\\", \\\"5622\\\", \\\"5623\\\", \\\"5624\\\", \\\"5625\\\", \\\"5626\\\"])\\n     ```  \\n   - Todos os valores devem ser strings.  \\n   - O JSON gerado deve ser v√°lido, sem quebras de linha ou conte√∫do adicional.  \\n\\n5. **Regras Adicionais:**  \\n   - **NUNCA** retorne NCM com menos de 4 n√∫meros.\\n   - Caso nenhum NCM seja encontrado, retorne:  \\n     ```json\\n     (\\\"NCM\\\": [])\\n     ```  \\n   - **N√£o inclua texto explicativo, coment√°rios ou qualquer conte√∫do fora do JSON.**  \\n\\n6. **Exemplo de Entrada e Sa√≠da:**  \\n   - Entrada:  \\n     ```json\\n     (\\\"UNICO\\\": [\\\"1001\\\", \\\"7002\\\"], \\\"INTERVALO\\\": [\\\"5622 a 5626\\\"])\\n     ```  \\n   - Retorno esperado:  \\n     ```json\\n     (\\\"NCM\\\": [\\\"1001\\\", \\\"5622\\\", \\\"5623\\\", \\\"5624\\\", \\\"5625\\\", \\\"5626\\\", \\\"7002\\\"])\\n     ```  \\n\\n7. **Valida√ß√£o e Verifica√ß√£o:**  \\n   - Antes de retornar, revise:  \\n     - Se todos os intervalos foram corretamente expandidos.  \\n     - Se o JSON est√° formatado conforme as regras.  \\n     - Se n√£o h√° duplica√ß√£o de valores na lista consolidada.  \\n\\n8. **Objetivo Final:**  \\n   - Retorne uma lista √∫nica e ordenada de NCMs, com todos os intervalos expandidos e sem redund√¢ncias, assegurando a conformidade com as regras e sem repetir valores de NCM.\",\"humanMessagePrompt\":\"Retorne apenas a lista completa de NCMs processados, garantindo que:  \\n1. O retorno seja no formato JSON, sem qualquer texto adicional.  \\n2. N√£o inclua quebras de linha no retorno.  \\n3. N√£o utilize o formato Markdown em nenhuma circunst√¢ncia.  \",\"messageHistory\":\"\",\"sequentialNode\":[\"{{seqLLMNode_0.data.instance}}\"],\"model\":\"\",\"promptValues\":\"{\\\"response\\\":\\\"{{seqLLMNode_0.data.instance}}\\\"}\",\"llmStructuredOutput\":\"\",\"updateStateMemory\":\"updateStateMemoryUI\"},\"outputAnchors\":[{\"id\":\"seqLLMNode_1-output-seqLLMNode-LLMNode\",\"name\":\"seqLLMNode\",\"label\":\"LLMNode\",\"description\":\"Run Chat Model and return the output\",\"type\":\"LLMNode\"}],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":451,\"selected\":true,\"positionAbsolute\":{\"x\":770.2872467476096,\"y\":265.7318010896166},\"dragging\":false},{\"id\":\"chatOpenAI_0\",\"position\":{\"x\":-368.92786962028094,\"y\":53.49891817134164},\"type\":\"customNode\",\"data\":{\"loadMethods\":{},\"label\":\"ChatOpenAI\",\"name\":\"chatOpenAI\",\"version\":7,\"type\":\"ChatOpenAI\",\"icon\":\"/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAI/openai.svg\",\"category\":\"Chat Models\",\"description\":\"Wrapper around OpenAI large language models that use the Chat endpoint\",\"baseClasses\":[\"ChatOpenAI\",\"BaseChatModel\",\"BaseLanguageModel\",\"Runnable\"],\"credential\":\"a0fffb09-1722-4f82-8532-e2bba4363b32\",\"inputs\":{\"cache\":\"\",\"modelName\":\"gpt-4o-mini\",\"temperature\":\"0\",\"maxTokens\":\"\",\"topP\":\"\",\"frequencyPenalty\":\"\",\"presencePenalty\":\"\",\"timeout\":\"\",\"basepath\":\"\",\"proxyUrl\":\"\",\"stopSequence\":\"\",\"baseOptions\":\"\",\"allowImageUploads\":\"\",\"imageResolution\":\"low\"},\"filePath\":\"/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAI/ChatOpenAI.js\",\"inputAnchors\":[{\"label\":\"Cache\",\"name\":\"cache\",\"type\":\"BaseCache\",\"optional\":true,\"id\":\"chatOpenAI_0-input-cache-BaseCache\"}],\"inputParams\":[{\"label\":\"Connect Credential\",\"name\":\"credential\",\"type\":\"credential\",\"credentialNames\":[\"openAIApi\"],\"id\":\"chatOpenAI_0-input-credential-credential\"},{\"label\":\"Model Name\",\"name\":\"modelName\",\"type\":\"asyncOptions\",\"loadMethod\":\"listModels\",\"default\":\"gpt-3.5-turbo\",\"id\":\"chatOpenAI_0-input-modelName-asyncOptions\"},{\"label\":\"Temperature\",\"name\":\"temperature\",\"type\":\"number\",\"step\":0.1,\"default\":0.9,\"optional\":true,\"id\":\"chatOpenAI_0-input-temperature-number\"},{\"label\":\"Max Tokens\",\"name\":\"maxTokens\",\"type\":\"number\",\"step\":1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-maxTokens-number\"},{\"label\":\"Top Probability\",\"name\":\"topP\",\"type\":\"number\",\"step\":0.1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-topP-number\"},{\"label\":\"Frequency Penalty\",\"name\":\"frequencyPenalty\",\"type\":\"number\",\"step\":0.1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-frequencyPenalty-number\"},{\"label\":\"Presence Penalty\",\"name\":\"presencePenalty\",\"type\":\"number\",\"step\":0.1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-presencePenalty-number\"},{\"label\":\"Timeout\",\"name\":\"timeout\",\"type\":\"number\",\"step\":1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-timeout-number\"},{\"label\":\"BasePath\",\"name\":\"basepath\",\"type\":\"string\",\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-basepath-string\"},{\"label\":\"Proxy Url\",\"name\":\"proxyUrl\",\"type\":\"string\",\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-proxyUrl-string\"},{\"label\":\"Stop Sequence\",\"name\":\"stopSequence\",\"type\":\"string\",\"rows\":4,\"optional\":true,\"description\":\"List of stop words to use when generating. Use comma to separate multiple stop words.\",\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-stopSequence-string\"},{\"label\":\"BaseOptions\",\"name\":\"baseOptions\",\"type\":\"json\",\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-baseOptions-json\"},{\"label\":\"Allow Image Uploads\",\"name\":\"allowImageUploads\",\"type\":\"boolean\",\"description\":\"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\"default\":false,\"optional\":true,\"id\":\"chatOpenAI_0-input-allowImageUploads-boolean\"},{\"label\":\"Image Resolution\",\"description\":\"This parameter controls the resolution in which the model views the image.\",\"name\":\"imageResolution\",\"type\":\"options\",\"options\":[{\"label\":\"Low\",\"name\":\"low\"},{\"label\":\"High\",\"name\":\"high\"},{\"label\":\"Auto\",\"name\":\"auto\"}],\"default\":\"low\",\"optional\":false,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-imageResolution-options\"}],\"outputs\":{},\"outputAnchors\":[{\"id\":\"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\"name\":\"chatOpenAI\",\"label\":\"ChatOpenAI\",\"description\":\"Wrapper around OpenAI large language models that use the Chat endpoint\",\"type\":\"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"}],\"id\":\"chatOpenAI_0\",\"selected\":false},\"width\":300,\"height\":670,\"selected\":false,\"positionAbsolute\":{\"x\":-368.92786962028094,\"y\":53.49891817134164},\"dragging\":false}],\"edges\":[{\"source\":\"seqStart_0\",\"sourceHandle\":\"seqStart_0-output-seqStart-Start\",\"target\":\"seqLLMNode_0\",\"targetHandle\":\"seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\",\"type\":\"buttonedge\",\"id\":\"seqStart_0-seqStart_0-output-seqStart-Start-seqLLMNode_0-seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"source\":\"seqLLMNode_1\",\"sourceHandle\":\"seqLLMNode_1-output-seqLLMNode-LLMNode\",\"target\":\"seqEnd_0\",\"targetHandle\":\"seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode\",\"type\":\"buttonedge\",\"id\":\"seqLLMNode_1-seqLLMNode_1-output-seqLLMNode-LLMNode-seqEnd_0-seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode\"},{\"source\":\"seqLLMNode_0\",\"sourceHandle\":\"seqLLMNode_0-output-seqLLMNode-LLMNode\",\"target\":\"seqLLMNode_1\",\"targetHandle\":\"seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\",\"type\":\"buttonedge\",\"id\":\"seqLLMNode_0-seqLLMNode_0-output-seqLLMNode-LLMNode-seqLLMNode_1-seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"source\":\"chatOpenAI_0\",\"sourceHandle\":\"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\"target\":\"seqStart_0\",\"targetHandle\":\"seqStart_0-input-model-BaseChatModel\",\"type\":\"buttonedge\",\"id\":\"chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-seqStart_0-seqStart_0-input-model-BaseChatModel\"}],\"viewport\":{\"x\":321.5921883112017,\"y\":58.16855746807454,\"zoom\":0.6996967811684826}}",
  "followUpPrompts": null,
  "id": "8cc34b7b-bfcf-4bcb-a1ef-a5fa0304f898",
  "isPublic": false,
  "name": "An√°lise Critica - [ 6 - Lista NCMs em Lista Negativa SC ]",
  "repo_name": "n8n-test",
  "repo_owner": "VitorNakagawaL",
  "repo_path": "flowise-workflows/",
  "speechToText": null,
  "type": "MULTIAGENT",
  "updatedDate": "2025-02-26T13:42:33.850Z"
}