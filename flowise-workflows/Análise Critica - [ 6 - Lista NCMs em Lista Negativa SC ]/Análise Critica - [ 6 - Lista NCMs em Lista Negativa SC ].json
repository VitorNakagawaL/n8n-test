{
  "analytic": "{\"langSmith\":{\"credentialId\":\"9fb56a6f-2c15-486c-9125-6fd27091d002\",\"projectName\":\"INTERSEAS - Lista Negativa NCM SC\",\"status\":true}}",
  "apiConfig": null,
  "apikeyid": null,
  "category": "Prod",
  "chatbotConfig": null,
  "createdDate": "2024-12-16T14:28:59.274Z",
  "deployed": false,
  "flowData": "{\"nodes\":[{\"id\":\"seqStart_0\",\"position\":{\"x\":21.45392570982736,\"y\":340.13358779696284},\"type\":\"customNode\",\"data\":{\"id\":\"seqStart_0\",\"label\":\"Start\",\"version\":2,\"name\":\"seqStart\",\"type\":\"Start\",\"baseClasses\":[\"Start\"],\"category\":\"Sequential Agents\",\"description\":\"Starting point of the conversation\",\"inputParams\":[],\"inputAnchors\":[{\"label\":\"Chat Model\",\"name\":\"model\",\"type\":\"BaseChatModel\",\"description\":\"Only compatible with models that are capable of function calling: ChatOpenAI, ChatMistral, ChatAnthropic, ChatGoogleGenerativeAI, ChatVertexAI, GroqChat\",\"id\":\"seqStart_0-input-model-BaseChatModel\"},{\"label\":\"Agent Memory\",\"name\":\"agentMemory\",\"type\":\"BaseCheckpointSaver\",\"description\":\"Save the state of the agent\",\"optional\":true,\"id\":\"seqStart_0-input-agentMemory-BaseCheckpointSaver\"},{\"label\":\"State\",\"name\":\"state\",\"type\":\"State\",\"description\":\"State is an object that is updated by nodes in the graph, passing from one node to another. By default, state contains \\\"messages\\\" that got updated with each message sent and received.\",\"optional\":true,\"id\":\"seqStart_0-input-state-State\"},{\"label\":\"Input Moderation\",\"description\":\"Detect text that could generate harmful output and prevent it from being sent to the language model\",\"name\":\"inputModeration\",\"type\":\"Moderation\",\"optional\":true,\"list\":true,\"id\":\"seqStart_0-input-inputModeration-Moderation\"}],\"inputs\":{\"model\":\"{{chatOpenAI_0.data.instance}}\",\"agentMemory\":\"\",\"state\":\"\",\"inputModeration\":\"\"},\"outputAnchors\":[{\"id\":\"seqStart_0-output-seqStart-Start\",\"name\":\"seqStart\",\"label\":\"Start\",\"description\":\"Starting point of the conversation\",\"type\":\"Start\"}],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":383,\"positionAbsolute\":{\"x\":21.45392570982736,\"y\":340.13358779696284},\"selected\":false,\"dragging\":false},{\"id\":\"seqEnd_0\",\"position\":{\"x\":1144.8448148439054,\"y\":573.7304138781866},\"type\":\"customNode\",\"data\":{\"id\":\"seqEnd_0\",\"label\":\"End\",\"version\":2,\"name\":\"seqEnd\",\"type\":\"End\",\"baseClasses\":[\"End\"],\"category\":\"Sequential Agents\",\"description\":\"End conversation\",\"inputParams\":[],\"inputAnchors\":[{\"label\":\"Agent | Condition | LLM | Tool Node\",\"name\":\"sequentialNode\",\"type\":\"Agent | Condition | LLMNode | ToolNode\",\"id\":\"seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode\"}],\"inputs\":{\"sequentialNode\":\"{{seqLLMNode_1.data.instance}}\"},\"outputAnchors\":[],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":143,\"selected\":false,\"positionAbsolute\":{\"x\":1144.8448148439054,\"y\":573.7304138781866},\"dragging\":false},{\"id\":\"seqLLMNode_0\",\"position\":{\"x\":395.3651580311179,\"y\":267.5533381510083},\"type\":\"customNode\",\"data\":{\"id\":\"seqLLMNode_0\",\"label\":\"LLM Node\",\"version\":3,\"name\":\"seqLLMNode\",\"type\":\"LLMNode\",\"baseClasses\":[\"LLMNode\"],\"category\":\"Sequential Agents\",\"description\":\"Run Chat Model and return the output\",\"inputParams\":[{\"label\":\"Name\",\"name\":\"llmNodeName\",\"type\":\"string\",\"placeholder\":\"LLM\",\"id\":\"seqLLMNode_0-input-llmNodeName-string\"},{\"label\":\"System Prompt\",\"name\":\"systemMessagePrompt\",\"type\":\"string\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-systemMessagePrompt-string\"},{\"label\":\"Human Prompt\",\"name\":\"humanMessagePrompt\",\"type\":\"string\",\"description\":\"This prompt will be added at the end of the messages as human message\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-humanMessagePrompt-string\"},{\"label\":\"Messages History\",\"name\":\"messageHistory\",\"description\":\"Return a list of messages between System Prompt and Human Prompt. This is useful when you want to provide few shot examples\",\"type\":\"code\",\"hideCodeExecute\":true,\"codeExample\":\"const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\\n\\nreturn [\\n    new HumanMessage(\\\"What is 333382 ðŸ¦œ 1932?\\\"),\\n    new AIMessage({\\n        content: \\\"\\\",\\n        tool_calls: [\\n        {\\n            id: \\\"12345\\\",\\n            name: \\\"calulator\\\",\\n            args: {\\n                number1: 333382,\\n                number2: 1932,\\n                operation: \\\"divide\\\",\\n            },\\n        },\\n        ],\\n    }),\\n    new ToolMessage({\\n        tool_call_id: \\\"12345\\\",\\n        content: \\\"The answer is 172.558.\\\",\\n    }),\\n    new AIMessage(\\\"The answer is 172.558.\\\"),\\n]\",\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-messageHistory-code\"},{\"label\":\"Format Prompt Values\",\"name\":\"promptValues\",\"description\":\"Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value\",\"type\":\"json\",\"optional\":true,\"acceptVariable\":true,\"list\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-promptValues-json\"},{\"label\":\"JSON Structured Output\",\"name\":\"llmStructuredOutput\",\"type\":\"datagrid\",\"description\":\"Instruct the LLM to give output in a JSON structured schema\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"editable\":true},{\"field\":\"type\",\"headerName\":\"Type\",\"type\":\"singleSelect\",\"valueOptions\":[\"String\",\"String Array\",\"Number\",\"Boolean\",\"Enum\"],\"editable\":true},{\"field\":\"enumValues\",\"headerName\":\"Enum Values\",\"editable\":true},{\"field\":\"description\",\"headerName\":\"Description\",\"flex\":1,\"editable\":true}],\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_0-input-llmStructuredOutput-datagrid\"},{\"label\":\"Update State\",\"name\":\"updateStateMemory\",\"type\":\"tabs\",\"tabIdentifier\":\"selectedUpdateStateMemoryTab\",\"default\":\"updateStateMemoryUI\",\"additionalParams\":true,\"tabs\":[{\"label\":\"Update State (Table)\",\"name\":\"updateStateMemoryUI\",\"type\":\"datagrid\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Key and value pair to be updated. For example: if you have the following State:\\n    | Key       | Operation     | Default Value     |\\n    |-----------|---------------|-------------------|\\n    | user      | Replace       |                   |\\n\\n    You can update the \\\"user\\\" value with the following:\\n    | Key       | Value     |\\n    |-----------|-----------|\\n    | user      | john doe  |\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can do the following:\\n    | Key       | Value                     |\\n    |-----------|---------------------------|\\n    | user      | `$flow.output.content`  |\\n\\n3. You can get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"type\":\"asyncSingleSelect\",\"loadMethod\":\"loadStateKeys\",\"flex\":0.5,\"editable\":true},{\"field\":\"value\",\"headerName\":\"Value\",\"type\":\"freeSolo\",\"valueOptions\":[{\"label\":\"LLM Node Output (string)\",\"value\":\"$flow.output.content\"},{\"label\":\"LLM JSON Output Key (string)\",\"value\":\"$flow.output.<replace-with-key>\"},{\"label\":\"Global variable (string)\",\"value\":\"$vars.<variable-name>\"},{\"label\":\"Input Question (string)\",\"value\":\"$flow.input\"},{\"label\":\"Session Id (string)\",\"value\":\"$flow.sessionId\"},{\"label\":\"Chat Id (string)\",\"value\":\"$flow.chatId\"},{\"label\":\"Chatflow Id (string)\",\"value\":\"$flow.chatflowId\"}],\"editable\":true,\"flex\":1}],\"optional\":true,\"additionalParams\":true},{\"label\":\"Update State (Code)\",\"name\":\"updateStateMemoryCode\",\"type\":\"code\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Return the key value JSON object. For example: if you have the following State:\\n    ```json\\n    {\\n        \\\"user\\\": null\\n    }\\n    ```\\n\\n    You can update the \\\"user\\\" value by returning the following:\\n    ```js\\n    return {\\n        \\\"user\\\": \\\"john doe\\\"\\n    }\\n    ```\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can return the following:\\n    ```js\\n    return {\\n        \\\"user\\\": $flow.output.content\\n    }\\n    ```\\n\\n3. You can also get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values. Must return an object representing the state\",\"hideCodeExecute\":true,\"codeExample\":\"const result = $flow.output;\\n\\n/* Suppose we have a custom State schema like this:\\n* {\\n    aggregate: {\\n        value: (x, y) => x.concat(y),\\n        default: () => []\\n    }\\n  }\\n*/\\n\\nreturn {\\n  aggregate: [result.content]\\n};\",\"optional\":true,\"additionalParams\":true}],\"id\":\"seqLLMNode_0-input-updateStateMemory-tabs\"}],\"inputAnchors\":[{\"label\":\"Start | Agent | Condition | LLM | Tool Node\",\"name\":\"sequentialNode\",\"type\":\"Start | Agent | Condition | LLMNode | ToolNode\",\"list\":true,\"id\":\"seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"label\":\"Chat Model\",\"name\":\"model\",\"type\":\"BaseChatModel\",\"optional\":true,\"description\":\"Overwrite model to be used for this node\",\"id\":\"seqLLMNode_0-input-model-BaseChatModel\"}],\"inputs\":{\"llmNodeName\":\"agent_pesquisador\",\"systemMessagePrompt\":\"**FunÃ§Ã£o do Agente:**  \\nSua tarefa Ã© analisar o texto recebido na entrada `{question}`, extraÃ­do de um decreto, e identificar **todos os NCMs mencionados** na **Lista de Mercadorias Importadas NÃ£o AlcanÃ§adas por BenefÃ­cios Fiscais**, separando-os em **valores Ãºnicos** e **intervalos**, conforme necessÃ¡rio.  \\n\\n1. **IdentificaÃ§Ã£o de NCMs:**  \\n   - Considere como NCM apenas sequÃªncias que atendam aos seguintes critÃ©rios:  \\n     - **Possuem 4 ou mais dÃ­gitos consecutivos**, com ou sem pontos como separadores.  \\n     - **Devem ser tratados como um Ãºnico NCM**, mesmo que contenham pontos.  \\n     - Exemplos vÃ¡lidos:  \\n       - `\\\"1001\\\"`, `\\\"10.25\\\"`, `\\\"1001.25\\\"`, `\\\"70.13\\\"`, `\\\"2710.9\\\"`, `\\\"3826.00.00\\\"`.  \\n     - Exemplos invÃ¡lidos (**NUNCA devem ser considerados ou retornados**):  \\n       - `\\\"1\\\"`, `\\\"12\\\"`, `\\\"123\\\"`, `\\\"12.3\\\"`, `\\\"1.23\\\"` (menos de 4 dÃ­gitos consecutivos).  \\n   - Identifique **intervalos de NCMs** quando dois valores vÃ¡lidos forem conectados por palavras como `\\\"a\\\"`, `\\\"atÃ©\\\"`, ou similares.  \\n     - Para considerar um intervalo vÃ¡lido:  \\n       - Ambos os limites do intervalo devem ser sequÃªncias vÃ¡lidas de NCMs (com 4 ou mais dÃ­gitos).  \\n       - O texto deve conter termos como `\\\"NCM\\\"`, `\\\"NCMs\\\"` ou expressÃµes similares, antes ou depois do intervalo.  \\n     - Exemplos vÃ¡lidos de intervalos:  \\n       - `\\\"1001 a 1006\\\"`, `\\\"10.25 a 10.50\\\"`, `\\\"70.13 a 70.20\\\"`, `\\\"2710.9 a 2711.5\\\"`, `\\\"3826.00.00 a 3826.00.05\\\"`.  \\n     - Exemplos invÃ¡lidos de intervalos (nÃ£o devem ser considerados):  \\n       - `\\\"1 a 3\\\"`, `\\\"12 a 15\\\"`, `\\\"17 a 19\\\"` (valores com menos de 4 dÃ­gitos consecutivos).  \\n\\n2. **ClassificaÃ§Ã£o:**  \\n   - Separe os NCMs encontrados em dois grupos:  \\n     - **UNICO**: Apenas NCMs Ãºnicos (nÃ£o presentes dentro de intervalos).  \\n     - **INTERVALO**: Apenas intervalos de NCMs.  \\n   - **NÃ£o duplique informaÃ§Ãµes**:  \\n     - Se um NCM Ãºnico estiver contido em um intervalo, liste apenas o intervalo em \\\"INTERVALO\\\".  \\n\\n3. **Regras de FormataÃ§Ã£o do Retorno:**  \\n   - O retorno deve ser no formato JSON, seguindo a estrutura:  \\n     ```json\\n     (\\n       \\\"UNICO\\\": [\\\"1001\\\", \\\"10.25\\\"],\\n       \\\"INTERVALO\\\": [\\\"1003 a 1006\\\"]\\n     )\\n     ```  \\n   - Todos os valores devem ser strings, utilizando aspas duplas (`\\\"`).  \\n   - O retorno deve ser vÃ¡lido em JSON e **nÃ£o incluir quebras de linha** ou textos adicionais.  \\n\\n4. **Regras Adicionais:**  \\n   - **NUNCA** divida ou modifique intervalos encontrados; liste-os exatamente como aparecem.  \\n   - **NUNCA** retorne valores de NCMs repetidos\\n   - Caso nenhum NCM seja encontrado, retorne:  \\n     ```json\\n     (\\n       \\\"UNICO\\\": [],\\n       \\\"INTERVALO\\\": []\\n     )\\n     ```  \\n   - NÃ£o inclua textos explicativos, comentÃ¡rios ou informaÃ§Ãµes adicionais no retorno.  \\n\\n5. **Objetivo Final:**  \\n   - Extraia todos os NCMs mencionados na lista do decreto.  \\n   - Garanta que o retorno contenha exclusivamente NCMs classificados entre **valores Ãºnicos** e **intervalos**, respeitando as regras de formataÃ§Ã£o e validaÃ§Ã£o.\",\"humanMessagePrompt\":\"Retorne a lista de NCMs encontrados\",\"messageHistory\":\"\",\"sequentialNode\":[\"{{seqStart_0.data.instance}}\"],\"model\":\"\",\"promptValues\":\"{\\\"question\\\":\\\"{{question}}\\\"}\",\"llmStructuredOutput\":\"\",\"updateStateMemory\":\"updateStateMemoryUI\"},\"outputAnchors\":[{\"id\":\"seqLLMNode_0-output-seqLLMNode-LLMNode\",\"name\":\"seqLLMNode\",\"label\":\"LLMNode\",\"description\":\"Run Chat Model and return the output\",\"type\":\"LLMNode\"}],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":451,\"selected\":false,\"positionAbsolute\":{\"x\":395.3651580311179,\"y\":267.5533381510083},\"dragging\":false},{\"id\":\"seqLLMNode_1\",\"position\":{\"x\":770.2872467476096,\"y\":265.7318010896166},\"type\":\"customNode\",\"data\":{\"id\":\"seqLLMNode_1\",\"label\":\"LLM Node\",\"version\":3,\"name\":\"seqLLMNode\",\"type\":\"LLMNode\",\"baseClasses\":[\"LLMNode\"],\"category\":\"Sequential Agents\",\"description\":\"Run Chat Model and return the output\",\"inputParams\":[{\"label\":\"Name\",\"name\":\"llmNodeName\",\"type\":\"string\",\"placeholder\":\"LLM\",\"id\":\"seqLLMNode_1-input-llmNodeName-string\"},{\"label\":\"System Prompt\",\"name\":\"systemMessagePrompt\",\"type\":\"string\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-systemMessagePrompt-string\"},{\"label\":\"Human Prompt\",\"name\":\"humanMessagePrompt\",\"type\":\"string\",\"description\":\"This prompt will be added at the end of the messages as human message\",\"rows\":4,\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-humanMessagePrompt-string\"},{\"label\":\"Messages History\",\"name\":\"messageHistory\",\"description\":\"Return a list of messages between System Prompt and Human Prompt. This is useful when you want to provide few shot examples\",\"type\":\"code\",\"hideCodeExecute\":true,\"codeExample\":\"const { AIMessage, HumanMessage, ToolMessage } = require('@langchain/core/messages');\\n\\nreturn [\\n    new HumanMessage(\\\"What is 333382 ðŸ¦œ 1932?\\\"),\\n    new AIMessage({\\n        content: \\\"\\\",\\n        tool_calls: [\\n        {\\n            id: \\\"12345\\\",\\n            name: \\\"calulator\\\",\\n            args: {\\n                number1: 333382,\\n                number2: 1932,\\n                operation: \\\"divide\\\",\\n            },\\n        },\\n        ],\\n    }),\\n    new ToolMessage({\\n        tool_call_id: \\\"12345\\\",\\n        content: \\\"The answer is 172.558.\\\",\\n    }),\\n    new AIMessage(\\\"The answer is 172.558.\\\"),\\n]\",\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-messageHistory-code\"},{\"label\":\"Format Prompt Values\",\"name\":\"promptValues\",\"description\":\"Assign values to the prompt variables. You can also use $flow.state.<variable-name> to get the state value\",\"type\":\"json\",\"optional\":true,\"acceptVariable\":true,\"list\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-promptValues-json\"},{\"label\":\"JSON Structured Output\",\"name\":\"llmStructuredOutput\",\"type\":\"datagrid\",\"description\":\"Instruct the LLM to give output in a JSON structured schema\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"editable\":true},{\"field\":\"type\",\"headerName\":\"Type\",\"type\":\"singleSelect\",\"valueOptions\":[\"String\",\"String Array\",\"Number\",\"Boolean\",\"Enum\"],\"editable\":true},{\"field\":\"enumValues\",\"headerName\":\"Enum Values\",\"editable\":true},{\"field\":\"description\",\"headerName\":\"Description\",\"flex\":1,\"editable\":true}],\"optional\":true,\"additionalParams\":true,\"id\":\"seqLLMNode_1-input-llmStructuredOutput-datagrid\"},{\"label\":\"Update State\",\"name\":\"updateStateMemory\",\"type\":\"tabs\",\"tabIdentifier\":\"selectedUpdateStateMemoryTab\",\"default\":\"updateStateMemoryUI\",\"additionalParams\":true,\"tabs\":[{\"label\":\"Update State (Table)\",\"name\":\"updateStateMemoryUI\",\"type\":\"datagrid\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Key and value pair to be updated. For example: if you have the following State:\\n    | Key       | Operation     | Default Value     |\\n    |-----------|---------------|-------------------|\\n    | user      | Replace       |                   |\\n\\n    You can update the \\\"user\\\" value with the following:\\n    | Key       | Value     |\\n    |-----------|-----------|\\n    | user      | john doe  |\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can do the following:\\n    | Key       | Value                     |\\n    |-----------|---------------------------|\\n    | user      | `$flow.output.content`  |\\n\\n3. You can get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values\",\"datagrid\":[{\"field\":\"key\",\"headerName\":\"Key\",\"type\":\"asyncSingleSelect\",\"loadMethod\":\"loadStateKeys\",\"flex\":0.5,\"editable\":true},{\"field\":\"value\",\"headerName\":\"Value\",\"type\":\"freeSolo\",\"valueOptions\":[{\"label\":\"LLM Node Output (string)\",\"value\":\"$flow.output.content\"},{\"label\":\"LLM JSON Output Key (string)\",\"value\":\"$flow.output.<replace-with-key>\"},{\"label\":\"Global variable (string)\",\"value\":\"$vars.<variable-name>\"},{\"label\":\"Input Question (string)\",\"value\":\"$flow.input\"},{\"label\":\"Session Id (string)\",\"value\":\"$flow.sessionId\"},{\"label\":\"Chat Id (string)\",\"value\":\"$flow.chatId\"},{\"label\":\"Chatflow Id (string)\",\"value\":\"$flow.chatflowId\"}],\"editable\":true,\"flex\":1}],\"optional\":true,\"additionalParams\":true},{\"label\":\"Update State (Code)\",\"name\":\"updateStateMemoryCode\",\"type\":\"code\",\"hint\":{\"label\":\"How to use\",\"value\":\"\\n1. Return the key value JSON object. For example: if you have the following State:\\n    ```json\\n    {\\n        \\\"user\\\": null\\n    }\\n    ```\\n\\n    You can update the \\\"user\\\" value by returning the following:\\n    ```js\\n    return {\\n        \\\"user\\\": \\\"john doe\\\"\\n    }\\n    ```\\n\\n2. If you want to use the LLM Node's output as the value to update state, it is available as `$flow.output` with the following structure:\\n    ```json\\n    {\\n        \\\"content\\\": 'Hello! How can I assist you today?',\\n        \\\"name\\\": \\\"\\\",\\n        \\\"additional_kwargs\\\": {},\\n        \\\"response_metadata\\\": {},\\n        \\\"tool_calls\\\": [],\\n        \\\"invalid_tool_calls\\\": [],\\n        \\\"usage_metadata\\\": {}\\n    }\\n    ```\\n\\n    For example, if the output `content` is the value you want to update the state with, you can return the following:\\n    ```js\\n    return {\\n        \\\"user\\\": $flow.output.content\\n    }\\n    ```\\n\\n3. You can also get default flow config, including the current \\\"state\\\":\\n    - `$flow.sessionId`\\n    - `$flow.chatId`\\n    - `$flow.chatflowId`\\n    - `$flow.input`\\n    - `$flow.state`\\n\\n4. You can get custom variables: `$vars.<variable-name>`\\n\\n\"},\"description\":\"This is only applicable when you have a custom State at the START node. After agent execution, you might want to update the State values. Must return an object representing the state\",\"hideCodeExecute\":true,\"codeExample\":\"const result = $flow.output;\\n\\n/* Suppose we have a custom State schema like this:\\n* {\\n    aggregate: {\\n        value: (x, y) => x.concat(y),\\n        default: () => []\\n    }\\n  }\\n*/\\n\\nreturn {\\n  aggregate: [result.content]\\n};\",\"optional\":true,\"additionalParams\":true}],\"id\":\"seqLLMNode_1-input-updateStateMemory-tabs\"}],\"inputAnchors\":[{\"label\":\"Start | Agent | Condition | LLM | Tool Node\",\"name\":\"sequentialNode\",\"type\":\"Start | Agent | Condition | LLMNode | ToolNode\",\"list\":true,\"id\":\"seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"label\":\"Chat Model\",\"name\":\"model\",\"type\":\"BaseChatModel\",\"optional\":true,\"description\":\"Overwrite model to be used for this node\",\"id\":\"seqLLMNode_1-input-model-BaseChatModel\"}],\"inputs\":{\"llmNodeName\":\"agent\",\"systemMessagePrompt\":\"**FunÃ§Ã£o do Agente:**  \\nSua tarefa Ã© analisar a entrada: {response} e processar **todos os NCMs** mencionados, transformando intervalos em listas de valores individuais e gerando um retorno consolidado.  \\n\\n1. **IdentificaÃ§Ã£o de NCMs:**  \\n   - Os NCMs podem ser:  \\n     - **Valores Ãºnicos**: como `\\\"1001\\\"`.  \\n     - **Intervalos**: como `\\\"5622 a 5626\\\"`.  \\n   - Identifique intervalos que contenham dois nÃºmeros separados pela palavra `\\\"a\\\"`.  \\n\\n2. **TransformaÃ§Ã£o de Intervalos:**  \\n   - Para cada intervalo identificado (exemplo: `\\\"5622 a 5626\\\"`):  \\n     - Extraia os dois nÃºmeros (inÃ­cio e fim).  \\n     - Gere uma sequÃªncia de todos os nÃºmeros do intervalo, incluindo o inÃ­cio e o fim (exemplo: `\\\"5622 a 5626\\\"` â†’ `[\\\"5622\\\", \\\"5623\\\", \\\"5624\\\", \\\"5625\\\", \\\"5626\\\"]`).  \\n     - Substitua o intervalo original pela sequÃªncia gerada.  \\n   - **Nunca mantenha intervalos no formato original no retorno.**  \\n\\n3. **ConsolidaÃ§Ã£o dos Resultados:**  \\n   - Combine os valores Ãºnicos com as listas expandidas dos intervalos em uma Ãºnica lista consolidada.  \\n   - Garanta que a lista resultante esteja em ordem crescente (ordenada numericamente).  \\n\\n4. **Regras de Retorno:**  \\n   - O retorno deve ser no formato JSON, com a seguinte estrutura:  \\n     ```json\\n     (\\\"NCM\\\": [\\\"1001\\\", \\\"5622\\\", \\\"5623\\\", \\\"5624\\\", \\\"5625\\\", \\\"5626\\\"])\\n     ```  \\n   - Todos os valores devem ser strings.  \\n   - O JSON gerado deve ser vÃ¡lido, sem quebras de linha ou conteÃºdo adicional.  \\n\\n5. **Regras Adicionais:**  \\n   - **NUNCA** retorne NCM com menos de 4 nÃºmeros.\\n   - Caso nenhum NCM seja encontrado, retorne:  \\n     ```json\\n     (\\\"NCM\\\": [])\\n     ```  \\n   - **NÃ£o inclua texto explicativo, comentÃ¡rios ou qualquer conteÃºdo fora do JSON.**  \\n\\n6. **Exemplo de Entrada e SaÃ­da:**  \\n   - Entrada:  \\n     ```json\\n     (\\\"UNICO\\\": [\\\"1001\\\", \\\"7002\\\"], \\\"INTERVALO\\\": [\\\"5622 a 5626\\\"])\\n     ```  \\n   - Retorno esperado:  \\n     ```json\\n     (\\\"NCM\\\": [\\\"1001\\\", \\\"5622\\\", \\\"5623\\\", \\\"5624\\\", \\\"5625\\\", \\\"5626\\\", \\\"7002\\\"])\\n     ```  \\n\\n7. **ValidaÃ§Ã£o e VerificaÃ§Ã£o:**  \\n   - Antes de retornar, revise:  \\n     - Se todos os intervalos foram corretamente expandidos.  \\n     - Se o JSON estÃ¡ formatado conforme as regras.  \\n     - Se nÃ£o hÃ¡ duplicaÃ§Ã£o de valores na lista consolidada.  \\n\\n8. **Objetivo Final:**  \\n   - Retorne uma lista Ãºnica e ordenada de NCMs, com todos os intervalos expandidos e sem redundÃ¢ncias, assegurando a conformidade com as regras e sem repetir valores de NCM.\",\"humanMessagePrompt\":\"Retorne apenas a lista completa de NCMs processados, garantindo que:  \\n1. O retorno seja no formato JSON, sem qualquer texto adicional.  \\n2. NÃ£o inclua quebras de linha no retorno.  \\n3. NÃ£o utilize o formato Markdown em nenhuma circunstÃ¢ncia.  \",\"messageHistory\":\"\",\"sequentialNode\":[\"{{seqLLMNode_0.data.instance}}\"],\"model\":\"\",\"promptValues\":\"{\\\"response\\\":\\\"{{seqLLMNode_0.data.instance}}\\\"}\",\"llmStructuredOutput\":\"\",\"updateStateMemory\":\"updateStateMemoryUI\"},\"outputAnchors\":[{\"id\":\"seqLLMNode_1-output-seqLLMNode-LLMNode\",\"name\":\"seqLLMNode\",\"label\":\"LLMNode\",\"description\":\"Run Chat Model and return the output\",\"type\":\"LLMNode\"}],\"outputs\":{},\"selected\":false},\"width\":300,\"height\":451,\"selected\":true,\"positionAbsolute\":{\"x\":770.2872467476096,\"y\":265.7318010896166},\"dragging\":false},{\"id\":\"chatOpenAI_0\",\"position\":{\"x\":-368.92786962028094,\"y\":53.49891817134164},\"type\":\"customNode\",\"data\":{\"loadMethods\":{},\"label\":\"ChatOpenAI\",\"name\":\"chatOpenAI\",\"version\":7,\"type\":\"ChatOpenAI\",\"icon\":\"/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAI/openai.svg\",\"category\":\"Chat Models\",\"description\":\"Wrapper around OpenAI large language models that use the Chat endpoint\",\"baseClasses\":[\"ChatOpenAI\",\"BaseChatModel\",\"BaseLanguageModel\",\"Runnable\"],\"credential\":\"a0fffb09-1722-4f82-8532-e2bba4363b32\",\"inputs\":{\"cache\":\"\",\"modelName\":\"gpt-4o-mini\",\"temperature\":\"0\",\"maxTokens\":\"\",\"topP\":\"\",\"frequencyPenalty\":\"\",\"presencePenalty\":\"\",\"timeout\":\"\",\"basepath\":\"\",\"proxyUrl\":\"\",\"stopSequence\":\"\",\"baseOptions\":\"\",\"allowImageUploads\":\"\",\"imageResolution\":\"low\"},\"filePath\":\"/usr/local/lib/node_modules/flowise/node_modules/flowise-components/dist/nodes/chatmodels/ChatOpenAI/ChatOpenAI.js\",\"inputAnchors\":[{\"label\":\"Cache\",\"name\":\"cache\",\"type\":\"BaseCache\",\"optional\":true,\"id\":\"chatOpenAI_0-input-cache-BaseCache\"}],\"inputParams\":[{\"label\":\"Connect Credential\",\"name\":\"credential\",\"type\":\"credential\",\"credentialNames\":[\"openAIApi\"],\"id\":\"chatOpenAI_0-input-credential-credential\"},{\"label\":\"Model Name\",\"name\":\"modelName\",\"type\":\"asyncOptions\",\"loadMethod\":\"listModels\",\"default\":\"gpt-3.5-turbo\",\"id\":\"chatOpenAI_0-input-modelName-asyncOptions\"},{\"label\":\"Temperature\",\"name\":\"temperature\",\"type\":\"number\",\"step\":0.1,\"default\":0.9,\"optional\":true,\"id\":\"chatOpenAI_0-input-temperature-number\"},{\"label\":\"Max Tokens\",\"name\":\"maxTokens\",\"type\":\"number\",\"step\":1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-maxTokens-number\"},{\"label\":\"Top Probability\",\"name\":\"topP\",\"type\":\"number\",\"step\":0.1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-topP-number\"},{\"label\":\"Frequency Penalty\",\"name\":\"frequencyPenalty\",\"type\":\"number\",\"step\":0.1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-frequencyPenalty-number\"},{\"label\":\"Presence Penalty\",\"name\":\"presencePenalty\",\"type\":\"number\",\"step\":0.1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-presencePenalty-number\"},{\"label\":\"Timeout\",\"name\":\"timeout\",\"type\":\"number\",\"step\":1,\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-timeout-number\"},{\"label\":\"BasePath\",\"name\":\"basepath\",\"type\":\"string\",\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-basepath-string\"},{\"label\":\"Proxy Url\",\"name\":\"proxyUrl\",\"type\":\"string\",\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-proxyUrl-string\"},{\"label\":\"Stop Sequence\",\"name\":\"stopSequence\",\"type\":\"string\",\"rows\":4,\"optional\":true,\"description\":\"List of stop words to use when generating. Use comma to separate multiple stop words.\",\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-stopSequence-string\"},{\"label\":\"BaseOptions\",\"name\":\"baseOptions\",\"type\":\"json\",\"optional\":true,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-baseOptions-json\"},{\"label\":\"Allow Image Uploads\",\"name\":\"allowImageUploads\",\"type\":\"boolean\",\"description\":\"Automatically uses gpt-4-vision-preview when image is being uploaded from chat. Only works with LLMChain, Conversation Chain, ReAct Agent, Conversational Agent, Tool Agent\",\"default\":false,\"optional\":true,\"id\":\"chatOpenAI_0-input-allowImageUploads-boolean\"},{\"label\":\"Image Resolution\",\"description\":\"This parameter controls the resolution in which the model views the image.\",\"name\":\"imageResolution\",\"type\":\"options\",\"options\":[{\"label\":\"Low\",\"name\":\"low\"},{\"label\":\"High\",\"name\":\"high\"},{\"label\":\"Auto\",\"name\":\"auto\"}],\"default\":\"low\",\"optional\":false,\"additionalParams\":true,\"id\":\"chatOpenAI_0-input-imageResolution-options\"}],\"outputs\":{},\"outputAnchors\":[{\"id\":\"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\"name\":\"chatOpenAI\",\"label\":\"ChatOpenAI\",\"description\":\"Wrapper around OpenAI large language models that use the Chat endpoint\",\"type\":\"ChatOpenAI | BaseChatModel | BaseLanguageModel | Runnable\"}],\"id\":\"chatOpenAI_0\",\"selected\":false},\"width\":300,\"height\":670,\"selected\":false,\"positionAbsolute\":{\"x\":-368.92786962028094,\"y\":53.49891817134164},\"dragging\":false}],\"edges\":[{\"source\":\"seqStart_0\",\"sourceHandle\":\"seqStart_0-output-seqStart-Start\",\"target\":\"seqLLMNode_0\",\"targetHandle\":\"seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\",\"type\":\"buttonedge\",\"id\":\"seqStart_0-seqStart_0-output-seqStart-Start-seqLLMNode_0-seqLLMNode_0-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"source\":\"seqLLMNode_1\",\"sourceHandle\":\"seqLLMNode_1-output-seqLLMNode-LLMNode\",\"target\":\"seqEnd_0\",\"targetHandle\":\"seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode\",\"type\":\"buttonedge\",\"id\":\"seqLLMNode_1-seqLLMNode_1-output-seqLLMNode-LLMNode-seqEnd_0-seqEnd_0-input-sequentialNode-Agent | Condition | LLMNode | ToolNode\"},{\"source\":\"seqLLMNode_0\",\"sourceHandle\":\"seqLLMNode_0-output-seqLLMNode-LLMNode\",\"target\":\"seqLLMNode_1\",\"targetHandle\":\"seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\",\"type\":\"buttonedge\",\"id\":\"seqLLMNode_0-seqLLMNode_0-output-seqLLMNode-LLMNode-seqLLMNode_1-seqLLMNode_1-input-sequentialNode-Start | Agent | Condition | LLMNode | ToolNode\"},{\"source\":\"chatOpenAI_0\",\"sourceHandle\":\"chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable\",\"target\":\"seqStart_0\",\"targetHandle\":\"seqStart_0-input-model-BaseChatModel\",\"type\":\"buttonedge\",\"id\":\"chatOpenAI_0-chatOpenAI_0-output-chatOpenAI-ChatOpenAI|BaseChatModel|BaseLanguageModel|Runnable-seqStart_0-seqStart_0-input-model-BaseChatModel\"}],\"viewport\":{\"x\":321.5921883112017,\"y\":58.16855746807454,\"zoom\":0.6996967811684826}}",
  "followUpPrompts": null,
  "id": "8cc34b7b-bfcf-4bcb-a1ef-a5fa0304f898",
  "isPublic": false,
  "name": "AnÃ¡lise Critica - [ 6 - Lista NCMs em Lista Negativa SC ]",
  "repo_name": "n8n-test",
  "repo_owner": "VitorNakagawaL",
  "repo_path": "flowise-workflows/",
  "speechToText": null,
  "type": "MULTIAGENT",
  "updatedDate": "2025-02-26T13:42:33.850Z"
}